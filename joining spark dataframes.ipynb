{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e85778-86e7-45c1-b5a5-454a521b509a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join 2 dataframes based on a key"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample DataFrame 1: Customers\n",
    "customers = spark.createDataFrame([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\")\n",
    "], [\"customer_id\", \"name\"])\n",
    "\n",
    "# Sample DataFrame 2: Orders\n",
    "orders = spark.createDataFrame([\n",
    "    (1, \"Laptop\"),\n",
    "    (2, \"Tablet\"),\n",
    "    (1, \"Phone\"),\n",
    "    (4, \"Monitor\")\n",
    "], [\"customer_id\", \"product\"])\n",
    "\n",
    "# Perform an inner join on 'customer_id'\n",
    "joined_df = customers.join(orders, on=\"customer_id\", how=\"left\")    # join types: inner, left, right, outer\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136a4961-1e76-4034-a3b4-6961151f6589",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Detect duplicate customers in a single dataframe"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data with duplicates\n",
    "data = [\n",
    "    (1, \"John Doe\", \"john.doe@example.com\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@example.com\"),\n",
    "    (3, \"John Doe\", \"john.doe@example.com\"),\n",
    "    (4, \"Alice Johnson\", \"alice.johnson@example.com\"),\n",
    "    (5, \"Jane Smith\", \"jane.smith@example.com\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"CustomerID\", \"Name\", \"Email\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Detect duplicates based on 'Name' and 'Email'\n",
    "duplicates = df.groupBy(\"Name\", \"Email\").count().filter(col(\"count\") > 1)\n",
    "\n",
    "# Show the duplicates\n",
    "print(\"Duplicate Records:\")\n",
    "duplicates.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d16856e-5013-4ad6-80de-74626035b17c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join 2 customer dataframes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Sample data for DataFrame 1\n",
    "data1 = [\n",
    "    (1, \"John Doe\", \"john.doe@example.com\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@example.com\"),\n",
    "    (3, \"Alice Johnson\", \"alice.johnson@example.com\")\n",
    "]\n",
    "\n",
    "# Sample data for DataFrame 2\n",
    "data2 = [\n",
    "    (4, \"John Doe\", \"john.doe@example.com\"),\n",
    "    (5, \"Jane Smith\", \"jane.smith@example.com\"),\n",
    "    (6, \"Bob Brown\", \"bob.brown@example.com\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"CustomerID\", \"Name\", \"Email\"]\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = spark.createDataFrame(data1, schema)\n",
    "df2 = spark.createDataFrame(data2, schema)\n",
    "\n",
    "# Perform inner join to find customers in both DataFrames\n",
    "joined_df = df1.join(df2, on=[\"Name\", \"Email\"], how=\"inner\")\n",
    "\n",
    "# Show the result\n",
    "joined_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d5dfad-e163-4bdd-a669-9a38970b3ad4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Levenshtein distance"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Sample data for DataFrame 1\n",
    "data1 = [\n",
    "    (1, \"John Doe\", \"john.doe@example.com\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@example.com\"),\n",
    "    (3, \"Alice Johnson\", \"alice.johnson@example.com\")\n",
    "]\n",
    "\n",
    "# Sample data for DataFrame 2\n",
    "data2 = [\n",
    "    (4, \"Jon Doe\", \"john.doe@example.com\"),\n",
    "    (5, \"Jane Smyth\", \"jane.smith@example.com\"),\n",
    "    (6, \"Bob Brown\", \"bob.brown@example.com\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"CustomerID\", \"Name\", \"Email\"]\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = spark.createDataFrame(data1, schema)\n",
    "df2 = spark.createDataFrame(data2, schema)\n",
    "\n",
    "# Rename columns to avoid ambiguity\n",
    "df1 = df1.withColumnRenamed(\"CustomerID\", \"CustomerID1\") \\\n",
    "         .withColumnRenamed(\"Name\", \"Name1\") \\\n",
    "         .withColumnRenamed(\"Email\", \"Email1\")\n",
    "\n",
    "df2 = df2.withColumnRenamed(\"CustomerID\", \"CustomerID2\") \\\n",
    "         .withColumnRenamed(\"Name\", \"Name2\") \\\n",
    "         .withColumnRenamed(\"Email\", \"Email2\")\n",
    "\n",
    "# Perform fuzzy join using Levenshtein distance on 'Name'\n",
    "fuzzy_joined_df = df1.crossJoin(df2).filter(expr(\"levenshtein(Name1, Name2) <= 2\"))  # less than 2 edits required to match\n",
    "\n",
    "# Show the result\n",
    "fuzzy_joined_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94e4652b-6e96-456b-9846-c19fb03f1db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Jaro Winkler demonstration\n",
    "\n",
    "Measures similarity between two strings based on:\n",
    "- The number of matching characters.\n",
    "- The number of transpositions (characters that match but are out of order).\n",
    "- Produces a score between 0 (no similarity) and 1 (exact match).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0b1d05-8f03-4610-9cb8-0bb7b57ccc9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install textdistance library"
    }
   },
   "outputs": [],
   "source": [
    "%pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca50fd3-3844-4907-a250-184b10684564",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "use jaro winkler algorithm"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import textdistance\n",
    "\n",
    "# Sample data for DataFrame 1\n",
    "data1 = [\n",
    "    (1, \"John Doe\", \"john.doe@example.com\"),\n",
    "    (2, \"Jane Smith\", \"jane.smith@example.com\"),\n",
    "    (3, \"Alice Johnson\", \"alice.johnson@example.com\")\n",
    "]\n",
    "\n",
    "# Sample data for DataFrame 2\n",
    "data2 = [\n",
    "    (4, \"Jon Doe\", \"john.doe@example.com\"),\n",
    "    (5, \"Jane Smyth\", \"jane.smith@example.com\"),\n",
    "    (6, \"Bob Brown\", \"bob.brown@example.com\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"CustomerID\", \"Name\", \"Email\"]\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = spark.createDataFrame(data1, schema)\n",
    "df2 = spark.createDataFrame(data2, schema)\n",
    "\n",
    "# Rename columns to avoid ambiguity\n",
    "df1 = df1.withColumnRenamed(\"CustomerID\", \"CustomerID1\") \\\n",
    "         .withColumnRenamed(\"Name\", \"Name1\") \\\n",
    "         .withColumnRenamed(\"Email\", \"Email1\")\n",
    "\n",
    "df2 = df2.withColumnRenamed(\"CustomerID\", \"CustomerID2\") \\\n",
    "         .withColumnRenamed(\"Name\", \"Name2\") \\\n",
    "         .withColumnRenamed(\"Email\", \"Email2\")\n",
    "\n",
    "# Define a UDF for Jaro-Winkler similarity\n",
    "def jaro_winkler_similarity(s1, s2):\n",
    "    if s1 is None or s2 is None:\n",
    "        return 0.0\n",
    "    return float(textdistance.jaro_winkler.normalized_similarity(s1, s2))\n",
    "\n",
    "jaro_winkler_udf = udf(jaro_winkler_similarity, DoubleType())\n",
    "\n",
    "# Perform cross join and apply Jaro-Winkler similarity UDF\n",
    "fuzzy_joined_df = df1.crossJoin(df2) \\\n",
    "    .withColumn(\"similarity\", jaro_winkler_udf(col(\"Name1\"), col(\"Name2\"))) \\\n",
    "    .filter(col(\"similarity\") >= 0.85)\n",
    "\n",
    "# Show the result\n",
    "fuzzy_joined_df.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "joining spark dataframes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
